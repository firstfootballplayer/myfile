智能指针，function bind，哈希map，移动move（避免引起内存重新分配和释放，调用move执行一次移动构造），lambda默认捕获是const（加mutable）



面向对象：
类中有虚方法时会生成一张虚表，它是一个函数指针数组，里面保存着每一个虚方法的函数指针，当派生类继承它时，会复制虚表，派生类
重写了虚方法后会覆盖掉基类中的虚方法指针，这些类的对象中含有一根虚指针指向各自对应的虚表，在运行时通过基类指针调用虚方法时
根据虚指针找到对应的虚表中的虚函数指针运行。


智能指针：
每个tcp连接用一个tcp连接对象来管理，有一个名为entry的结构体，它有一个tcp连接类的弱引用weakptr变量，维护一个固定长度队列，它的类型是hset，hset的类型是entry的强应用shared指针，当新的tcp连接建立时，
生成一个entry的强应用指针，同时将tcp连接对象绑定到entry内部的tcp连接弱引用上，再生成一个entry的弱引用来管理强应用，将它传入到tcp连接对象中，然后将entry强引用放入队列的队尾元素中。每当收到新消息和
心跳时由tcp连接类中的weakptr的lock函数，生成一个新的entry强引用将它放入队尾元素中，在间隔1秒的定时器回调函数中会定时将队首元素出队，并将新元素入队，
如果在7秒内没有收到新消息，则entry的强引用对全部出队，当最后一个元素出队时会调用它的析构函数，在析构函数中关闭该连接。

不使用相同的内置指针值初始化多个智能指针。
不delete get() 返回的指针。如果你使用get()返回的指针，记住当最后一个对应的智能指针销毁后，你的指针就会变为无效的。
用new创建一个对象的同时创建一个shared_ptr时，这时会发生两次动态申请内存：一次是给使用new申请的对象本身的，而另一次则是由shared_ptr的构造函数引发的为资源管理对象分配的。与此相反，当你使用make_shared
的时候，C++编译器只会一次性分配一个足够大的内存，用来保存这个资源管理者和这个新建对象
A对象中指针引用B对象，B对象指针引用A对象 （new出来的A的引用计数此时为1，b->m_a = a; //A的引用计数增加为2）只要引用链成环都会出现问题
不要在函数实参里创建shared_ptr，有缺陷可能的过程是先new int，然后调g()，g()发生异常，shared_ptr<int>没有创建，int内存泄露。



STL:
容器访问成员函数返回的是引用，可以改变元素。
迭代器失效的问题：
对于vector和string，如果容器内存被重新分配则全部失效，插入删除操作点之后失效，之前正常
deque如果操作点之前的元素较少，将操作点之前的所有元素向后移动一位，再操作第一个元素，向前移动将导致原迭代器中删除点及删除点之后的迭代器失效；向后移动将导致原迭代器中删除点及删除点之前的迭代器都失效
list，关联容器，操作只影响操作点。所有操作都只是针对节点移动指针，不会涉及到位置变化

map使用的内存更多，一个节点除了包含元素还有3个指针，指向父节点，左右子节点，而且，vector使用内存是连续的map则是链式的更容易分散，当数据结构很大时会跨越多个页面，这会导致更多的页面错误，使系统变慢。
添加元素用insert，更新已有元素用[]

算法参数优先用函数对象，函数对象被声明为内联，算法在实例化过程中将函数内联展开进行优化，而传入普通的函数时编译器会隐式将它转换为函数指针，算法在用到它时编译器都会产生一个间接的函数调用无法内联优化，



accept被系统调用中断：当阻塞于某个慢系统调用的一个进程捕获某个信号且相应信号处理函数返回时,该系统调用可能返回一个EINTR错误。有些内核自动重启某些被中断的系统调用。不过为了便于移植，当我们编写捕获信
号的程序时 (多数并发服务器捕获SIGCHLD) ,我们必须对慢系统调用返回 EINTR 有所准备
 if( (connfd = accept(listenfd, (SA *) &cliaddr, &clilen)) < 0){
            if(errno == EINTR)
                continue;
            else
                err_sys("accept error");
    }
注意，有一个函数我们不能重启: connect 。如果该函数返回 EINTR,我们就不能再次调用它，否则将立刻返回一个错误。当 connect 被一个捕获的信号中断而且不自动重启时，我们必须调用select来等待连接完成。

udp客户端收到端口不可达错误，这由sendto引起，sendto返回成功（只要发送缓冲区有空间就返回成功），这个异步错误只有在连接情况下才返回给它，
客户端通过 connect 绑定服务端的地址和端口，对 UDP 而言，可以有一定程度的性能提升。原因：
如果不使用 connect 方式，每次发送报文都会需要这样的过程：连接套接字→发送报文→断开套接字→连接套接字→发送报文→断开套接字 →………
如果使用 connect 方式，就会变成下面这样：连接套接字→发送报文→发送报文→……→最后断开套接字   （连接消耗三分之一udp传输）
多次调用connect：1，连接一个新的地址。2断开套接字





accpet返回前连接终止
当三路握手完成从而连接建立之后，客户TCP却发送了一个RST（复位）。在服务器端看来，就是该连接已由TCP排队，等待服务器进程调用accept时RST到达。稍后，服务器进程调用accept。
如何处理这种终止的连接依赖于不同的实现。源自Berkeley的实现完全在内核中处理中止的连接，服务器进程根本看不到。然而大多数的SVR4实现返回一个 EPROTO error值，而POSIX指出返回的errno值必须是
 ECONNABORTED。 POSIX作出修改的理由在于:流子系统(streams subsystem)中发生某些致命的协议相关事件时，也会返回EPROTO。

进程向一个接收到FIN的套接字写入消息，接收到RST应答，此时再次写入，内核会给进程发送一个SIGPIPE信号。此信号的缺省动作是终止进程，不产生core文件。不管进程是否捕捉该信号，写操作都会返回EPIPE错误。

服务器主机崩溃：当客户端和服务器之间建立连接后，服务器主机崩溃，此时通过网络发往服务器主机的消息不会得到任何回应。此时可能返回的错误有两种
1.客户端持续重传数据分节，一直没有得到回应，此时应返回ETIMEOUT错误。
2.如果中间路由器判断服务器主机不可达，并返回一个目的地不可达的ICMP错误，则错误是EHOSTUNREACH或ENETUNREACH。通过write的返回值，可以得知错误原因，但是通常需要等待很长一段时间。
如何检测崩溃，客户端持续主动发送（但要等很长时间），设置keepalive选项。

服务器主机崩溃后重启：当服务器主机崩溃后重启时，它的TCP丢失了崩溃前的所有连接信息，因此服务器TCP对于所收到的所有来自客户的数据分节相应以一个RST。
当客户TCP收到该RST时，客户正阻塞于readline调用，导致该调用返回ECONNRESET错误。


为什么IO多路复用要搭配非阻塞IO：io复用返回不等于read能读到
1.当数据达到socket缓冲区时，可能会因为一些原因被内核丢弃，比如，校验和错误，这时IO复用唤醒线程对socket读并不能读到数据，如果是阻塞IO就会被阻塞住。
2.多个进程accept同一个套接字时引发的惊群现象，只有一个连接到来，但是所有的监听进程都被唤醒了，但是最终只有一个进程可以accept到这个请求，其他进程在阻塞IO进程下都会被阻塞。
（linux2.6版本以后，linux内核已经解决了accept（）函数的“惊群”现象，大概的处理方式就是，当内核接收到一个客户连接后，只会唤醒等待队列上的第一个进程（线程））
epoll不一样，他监听的文件描述符，除了可能后续被accept调用外，还有可能是其他网络IO事件的，而其他IO事件是否只能由一个进程处理，是不一定的，内核不能保证这一点，这是一个由用
户决定的事情，例如可能一个文件会由多个进程来读写。所以，对epoll的惊群，内核则不予处理。（部分惊群）
3.epoll的ET模式必须要使用非阻塞IO，因为需要多次循环读写直到EAGAN出现，如果使用阻塞IO容易被阻塞住。



select、poll、epoll 区别总结：
首先select是posix支持的，而epoll是linux特定的系统调用，因此，epoll的可移植性就没有select好，但是考虑到epoll和select一般用作服务器的比较多，而服务器中大多又是linux，所以这个可移植性的影响应该不会很大。
支持一个进程所能打开的最大连接数：
select：单个进程所能打开的最大连接数有FD_SETSIZE宏定义大小是1024，当然我们可以对进行修改，然后重新编译内核，但是性能可能会受到影响，这需要进一步的测试。
poll：poll本质上和select没有区别，但是它没有最大连接数的限制，原因是它是基于链表来存储的。
epoll：虽然连接数有上限，但是很大，1G内存的机器上可以打开10万左右的连接，2G内存的机器可以打开20万左右的连接。

select用户自定义各个自己关心的描述符集合，将描述符添加到相应的集合中。然后调用select将集合中数据拷贝到内核中进行监控，监控原理：在内核中不断进行轮询遍历，判断哪个描述符就绪`可读就绪，当前任意一个
集合中有描述符就绪，则遍历完集合之后select调用返回，在返回之前将集合中所有未就绪的描述符从集合中移除，返回的是一个就绪描述符集合（内核态到用户态的拷贝）。用户需要通过判断哪个描述符还在集合中来判
断描述符是否就绪。因为集合在返回前被修改了，因此每次重新监控前需要重新添加集合中。有一次用户态到内核态的拷贝。

在调用epoll_create时，在内核cache中建了个红黑树用于存储传进来的fd，同时注册了回调函数，还会建立一个双向链表，用于存储就绪事件，内核在检测到描述符可读/可写时会调用回调函数将文件描述符放在就绪链表中
在调用epoll_wait时，判断双向链表有没有数据即可。有数据就返回，没有数据就sleep，等到timeout时间到后即使链表没数据也返回。所以，epoll_wait非常高效。
总结：
1select和poll传入fd集合时，每次都需要从用户态将fd集合传入内核态，epoll则只在第一次将fd添加到红黑树上，后续一直延用这棵树不会反复添加。
2内核态检测文件描述符状态的方式 ，select和poll采用轮询方式，遍历所有fd随着fd的增加效率线性下降;。epoll是根据每个fd上面的callback函数实现的;只有"活跃"的socket才会主动的去调用 callback函数，其他idle状态
socket则不会;epoll实现了一个"伪AIO，因为这时候推动力在os内核;
3就绪的文件描述符并传递给用户态的方式  需要在用户态遍历来判断。epoll_wait只用观察就绪链表中有无数据即可，有则将就绪的文件描述符放在传入的数组中并返回就绪的数量，用户只用依次处理即可。

epoll的边缘触发模式效率高，系统不会充斥大量不关心的就绪文件描述符
虽然epoll的性能最好，但是在连接数少并且连接都十分活跃的情况下，select和poll的性能可能比epoll好，毕竟epoll的通知机制需要很多函数回调。


四次挥手过程理解 ：
1）客户端进程发出连接释放报文，并且停止发送数据。释放数据报文首部，FIN=1，此时，客户端进入FIN-WAIT-1（终止等待1）状态。 
2）服务器收到连接释放报文，发出确认报文，ACK=1，ack=u+1，并且带上自己的序列号seq=v，此时，服务端就进入了CLOSE-WAIT（关闭等待）状态，这时候处于半关闭状态，即客户端已经没有数据要发送了，但是
服务器若发送数据，客户端依然要接受。这个状态还要持续一段时间，也就是整个CLOSE-WAIT状态持续的时间。
3）客户端收到服务器的确认请求后，此时，客户端就进入FIN-WAIT-2（终止等待2）状态，等待服务器发送连接释放报文（在这之前还需要接受服务器发送的最后的数据）。
4）服务器将最后的数据发送完毕后，就向客户端发送连接释放报文，FIN=1，ack=u+1，由于在半关闭状态，服务器很可能又发送了一些数据，假定此时的序列号为seq=w，此时，服务器就进入了LAST-ACK（最后确认）
状态，等待客户端的确认。
5）客户端收到服务器的连接释放报文后，必须发出确认，ACK=1，ack=w+1，而自己的序列号是seq=u+1，此时，客户端就进入了TIME-WAIT（时间等待）状态。注意此时TCP连接还没有释放，必须经过2∗∗MSL（最
长报文段寿命）的时间后，当客户端撤销相应的TCB后，才进入CLOSED状态。
6）服务器只要收到了客户端发出的确认，立即进入CLOSED状态。同样，撤销TCB后，就结束了这次的TCP连接。可以看到，服务器结束TCP连接的时间要比客户端早一些。

TIME_WAIT：
1这是因为TCP是建立在不可靠网络上的可靠协议。如果主动关闭的一端收到被动关闭一端的发出的FIN包后，返回ACK包，同时进入TIME_WAIT，但是由于网络的原因，主动关闭一端发送的ACK包可能会延迟，从而触发
被动关闭一方重传FIN包，这样一来一回极端情况正好是2MSL。如果主动关闭的一端直接close或者不到两倍MSL时间就关闭，那么被动关闭发出重传FIN包到达，可能出现的问题是：旧的连接不存在，系统只能返回RST包；
2为了使得数据包在网络中因过期而失效
 解析：假设没有time_wait状态时，A刚刚与B断开连接，C又以和A相同的ip和port和B建立起连接，TCP协议栈无法区分A和C是不同的连接， 这时，A本要发送给B的数据延迟到达之后会被现在的连接发来的数据进行处理，
实际上这是上一条连接的脏数据；所以在time_wait等待2MSL（报文在网络最大生存时间），将此连接的数据全部收到并丢弃，才能保证这些数据不会造成错误；

1.大量的短连接存在，在HTTP/1.0协议中默认使用短连接。
也就是说，浏览器和服务器每进行一次HTTP操作，就会建立一次连接，任务结束后就会断开连接，而断开连接这个请求是由server去发起的，主动关闭连接请求一端才会有TIME_WAIT状态连接
2.HTTP请求头里connection值被设置为close，如果HTTP请求中，connection的值被设置成close，那就相当于告诉server：server执行完HTTP请求后去主动关闭连接

TIME_WAIT状态过多有何危害？如何解决TIME_WAIT过多情况？
危害：tcp连接不能释放占用太多资源，阻塞其他正常连接；
解决方法
     第一种：可以设置SO_REUSEADDR套接字选项来通知内核，即使处于TIME_WAIT状态也可以立即重用端口。
     第二种：修改内核参数来调优（打开系统的TIMEWAIT重用和快速回收。）
     3将MSL值缩减，linux中MSL的值默认为60s，我们可以通过缩减MSL值来使得主动关闭连接一端由TIME_WAIT状态到关闭状态的时间减少，但是这样做会导致延迟报文无法清除以及主动关闭连接一端不能收到重传来的
     FIN请求，也会影响很多基于TCP的应用的连接复用和调优

大量CLOSE_WAIT：
Server 程序处于CLOSE_WAIT状态，而不是LAST_ACK状态，说明还没有发FIN给Client，那么可能是在关闭连接之前还有许多数据要发送或者其他事要做， 导致没有发这个FIN packet。  通常来说，一个CLOSE_WAIT会维
持至少2个小时的时间（这个时间外网服务器通常会做调整，要不然太危险了）。 
当对方调用closesocket的时候，我的程序正在调用recv中，这时候有可能对方发送的FIN包我没有收到，而是由TCP代回了一个ACK包，所以我这边套接字进入CLOSE_WAIT状态。 
所以建议在这里判断recv函数的返回值是否已出错，是的话就主动closesocket，这样防止没有接收到FIN包。 



tcp和udp：
每一条TCP连接只能是点到点的;UDP支持一对一，一对多，多对一和多对多的交互通信
tcp流式的，比如客户端2次分别发送200和300字节，服务端可能是一次接受500，可能第一次200，第二次200，第三次100，udp是数据报，客户端发送几次服务端肯定接受几次不会分包和组包，但是udp可能出现丢包或包序
错乱的情况所以udp是不可靠的，tcp则有完善的可靠性机制，同时因为tcp是面向连接的，所以会产生粘包问题需要应用层做处理
TCP对系统资源要求较多，UDP对系统资源要求较少。UDP没有发送缓冲区，因为UDP不保证可靠性，它没有重传机制，而TCP不同，TCP必须保证重新发送，所以必须要具备发送缓冲区。

TCP通过哪些措施，保证传输可靠：
连接管理 三次和四次
1、应用数据被分割成TCP认为最适合发送的数据块。这和UDP完全不同(将数据截断为合理的长度，数据块)
2、发送端利用定时器接受接受端的ack确认，否则将重发这个报文段。 (超时重传)
3、当TCP收到发自TCP连接另一端的数据，它将发送一个确认。这个确认不是立即发送，通常将推迟几分之一秒 (对于收到的请求，给出确认响应) (之所以推迟，可能是要对包做完整校验)（确认应答）
4、TCP将保持它首部和数据的检验和，目的是检测数据在传输过程中的任何变化。如果收到段的检验和有差错，TCP将丢弃这个报文段和不确认收到此报文段。 TCP发送数据端，超时会重发数据（校验机制）
5、既然TCP报文段作为IP数据报来传输，而IP数据报的到达可能会失序，因此TCP报文段的到达也可能会失序。如果必要，TCP将对收到的数据进行重新排序，将收到的数据以正确的顺序交给应用层。 (对失序数据进行重新排序，然后才交给应用层)
6、既然IP数据报会发生重复，TCP的接收端必须丢弃重复的数据。(对于重复数据，能够丢弃重复数据)
7、流量控制。拥塞控制（提高tcp性能，间接的可靠性）

拥塞控制：
对网络整体的控制，防止分组大量注入到网络中导致网络拥塞。若出现拥塞而不进行控制，整个网络的吞吐量将随输入负荷的增大而下降。

发送方维护一个拥塞窗口变量将其作为发送窗口，同时维护一个慢开始门限值。连接建立后拥塞窗口值设为1发送方发送一个数据收到确认后将窗口增加到2，成功发送2个数据后增加到4，如此循环成功后以指数增加窗口值，
当拥塞窗口达到慢开始门限值后，改用拥塞避免算法，这时每个传输轮次后拥塞窗口值线性的加1，之后如果出现了超时重传，发送方认为可能出现拥塞，将慢开始门限值改为发生拥塞时拥塞窗口的一半，将拥塞窗口改为1
重新执行慢开始。
快重传算法首先要求接收方每收到一个失序的报文段就立即发出重复确认（为的是使发送方及早的知道有报文段没有到达对方）而不要等到自己发送数据时才捎带确认。快重传算法规定，发送方只要一连收到三个重复确认
就应当立即重传，而不必等待超时重传。
快恢复：当发送方连续收到三个重复确认时，把慢开始门限减半。这是为了预防网络发生拥塞，然后开始执行拥塞避免算法。

流量控制
解决发送端与接收方吞吐量不匹配的问题，让发送方的发送速率不要太快，让接收方来得及接收。

流量控制引发的死锁？怎么避免死锁的发生？
当发送者收到了一个窗口为0的应答，发送者便停止发送，等待接收者的下一个应答。但是如果这个窗口不为0的应答在传输过程丢失，发送者一直等待下去，而接收者以为发送者已经收到该应答，等待接收新数据，这样
双方就相互等待，从而产生死锁。为了避免流量控制引发的死锁，TCP使用了持续计时器。每当发送者收到一个零窗口的应答后就启动该计时器。时间一到便主动发送报文询问接收者的窗口大小。若接收者仍然返回零窗口，
则重置该计时器继续等待；若窗口不为0，则表示应答报文丢失了，此时重置发送窗口后开始发送，这样就避免了死锁的产生。

利用滑动窗口机制实现对发送方的流量控制。窗口单位是字节，发送方的发送窗口不能超过接收方给出的接收窗口的数值。这里采用滑动窗口机制，一是不用每次发送完成都等到确认消息才发送，二是参考接收端的接收能力，限制发送数据段大小，避免丢失现象。
发送窗口在连接建立时由双方商定。但在通信的过程中，接收端可根据自己的资源情况，动态地调整对方的发送窗口。要达成TCP流量控制，需要满足下式：将未确认的数据量控制在值rwnd内：LastByteSent−LastByteAck≤rwnd



进程和线程：
进程是具有一定独立功能的程序关于某个数据集合上的一次运行活动，它是系统进行资源分配和调度的一个独立单位。例如，用户运行自己的程序，系统就创建一个进程，并为它分配资源，包括各种表格、内存空间、
磁盘空间、IO设备等，然后该进程被放入到进程的就绪队列，进程调度程序选中它，为它分配CPU及其他相关资源，该进程就被运行起来。 线程是进程的一个实体，是CPU调度和分配的基本单位，线程自己基本上不拥
有系统资源，只拥有一些在运行中必不可少的资源（如程序计数器、errno变量。一组寄存器和栈），但是，它可以与同属一个进程的其他的线程共享进程所拥有的全部资源。在没有实现线程的操作系统中，进程既是资源分配的基本
单位，又是调度的基本单位，它是系统中并发执行的单元。而在实现了线程的操作系统中，进程是资源分配的基本单位而线程是调度的基本单位，是系统中并发执行的单元。
Linux中线程借助进程机制实现（也称为轻量级进程），也有PCB，创建线程使用的底层函数和进程一样，都是clone。线程PCB中指向内存资源的三级页表（三级映射：进程PCB --> 页目录(可看成数组，首地址位于PCB中)
 --> 页表 --> 物理页面 --> 内存单元）和进程是相同的共享进程内存。
如果复制对方的地址空间，那么就产出一个“进程”；如果共享对方的地址空间，就产生一个“线程”。因此：Linux内核是不区分进程和线程的。只在用户层面上进行区分。所以，线程所有操作函数 pthread_* 是库函数，而非系统调用。
线程可分为用户态和内核态：用户态也称为协程 这也就是它为什么叫用户态的线程.内核态并不知道它的存在,创建、销毁，切换不需要系统调用比内核态线程快很多，用户态线程无法跨核心，一个进程的多个用户态线程不能并发，阻塞一个用户态线程会导致进程的主线程阻塞，直接交出执行权限
 
进程间通信方式有那些，管道，FIFO，信号，信号量，消息队列，共享内存（最快），套接字。线程同步：信号量，信号，互斥锁，条件变量加互斥锁，自旋锁


Linux内存管理：
代码段，数据段，BSS段，堆，栈
Linux操作系统采用虚拟内存管理技术，每个进程都有各自的大小为4G的线性虚拟空间，其中1g为内核空间，3g为用户空间，当进程切换，用户空间会变化；而内核空间由内核负责映射是固定的，每个进程的用户空间都是
完全独立、互不相干的。用户程序可使用比实际物理内存更大的地址空间
当进程分配内存，获得的是虚拟内存，实际的物理内存只有当进程真的访问时，会产生“缺页”异常，它会分配物理页，并建立对应的页表，虚拟地址映射到了物理内存上。（当然，如果页被换出到磁盘，也会产生缺页异常，不过这时不用再建立页表了）
Linux内核通过分页机制管理物理内存，它将整个内存划分成无数个4k（在i386体系结构中）大小的页，有助于灵活分配内存地址，采用伙伴关系算法减轻外部碎片（系统虽有足够的内存，但却是分散的碎片，无法满足对大块
“连续内存”的需求），slab技术减少内部碎片（系统为了满足一小段内存区（连续）的需要，不得不分配了一大区域连续内存给它，从而造成了空间浪费）。



malloc ：
Linux维护一个break指针，这个指针指向堆空间的某个地址。从堆起始地址到break之间的地址空间为映射好的，可以供进程访问；而从break往上，是未映射的地址空间，如果访问这段空间则程序会报错。我们用
malloc进行内存分配就是从break往上进行的。
malloc 函数维护一个空闲链表。 调用 malloc（）函数时，通过首次适应或最佳适应算法寻找一个满足要求的内存块。 然后，将该内存块一分为二（一块等于用户申请的大小，剩下的那块（如果有的话）返回到连接表上。
如果现有block都不能满足要求，则需要调用sbrk()在链表最后开辟一个新的block。（sbrk将break从当前位置移动increment所指定的增量成功时返回break移动之前所指向的地址，sbrk（0）返回当前break，brk将break指针直接设置为某个地址）
调用 free 函数时，它将用户释放的内存块连接到空闲链表上。 到最后，空闲链会被切成很多的小内存片段，将相邻的小空闲块合并成较大的内存块。glibc 的 free 实现中，只要堆顶附近释放总空间（包括合并的空间）超过 128k ，即会调用 sbrk(-SIZE) 来回溯堆顶指针，将原堆顶空间还给 OS 



内存池：
维护16个元素的指针数组，每个下标元素指向从8-128的自由链表，每块内存是一个union构造的单向链表管理的
使用allocate向内存池请求size大小的内存空间, 如果需要请求的内存大小大于128bytes, 直接使用malloc.
如果需要的内存大小小于128bytes, 根据size找到最适合的自由链表.
如果链表不为空, 返回第一个node, 链表头改为第二个node.
如果链表为空, 使用blockAlloc请求分配node.
如果内存池中有大于一个node的空间, 分配竟可能多的node(但是最多20个), 将一个node返回, 其他的node添加到链表中.
如果内存池只有一个node的空间, 直接返回给用户.
若果如果连一个node都没有, 再次向操作系统请求分配40个节点大小内存.
分配成功, 再次进行b过程
分配失败, 循环各个自由链表, 寻找空间
找到空间, 再次进行过程b
通过placementnew 完成对象的构造
用户调用deallocate释放内存空间, 调用析构函数后，如果要求释放的内存空间大于128bytes, 直接调用free.
否则按照其大小找到合适的自由链表, 并将其插入.


makefile：一个文本形式的文件，其中包含一些规则告诉make编译哪些文件以及怎样编译这些文件，每条规则包含以下内容：
一个target，即最终创建的东西
一个和多个dependencies列表，通常是编译目标文件所需要的其他文件
需要执行的一系列commands，用于从指定的相关文件创建目标文件
make执行时按顺序查找名为GNUmakefile，makefile或者Makefile文件，通常，大多数人常用Makefile












































